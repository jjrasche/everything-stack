import 'dart:async';

import 'timeout_config.dart';

/// Large language model service contract.
///
/// Provides streaming chat completion with conversational context.
///
/// ## Platform Support
/// - Native (Android/iOS/Desktop): ClaudeService (HTTP SSE streaming)
/// - Web: ClaudeService (HTTP SSE streaming via dart:html)
/// - Fallback: NullLLMService (no-op when not configured)
///
/// ## Usage
/// ```dart
/// // Initialize
/// await LLMService.instance.initialize();
///
/// // Chat with streaming response
/// final messages = [
///   Message(role: 'user', content: 'What is the capital of France?'),
/// ];
///
/// await for (final token in LLMService.instance.chat(
///   history: messages,
///   userMessage: 'And its population?',
/// )) {
///   print(token); // Stream tokens as they arrive
/// }
///
/// // Cleanup
/// LLMService.instance.dispose();
/// ```
///
/// ## Timeout Behavior
/// - **Connection timeout**: 15s to establish connection
/// - **Streaming idle timeout**: 10s without token â†’ assume connection stalled
/// - **No automatic retry**: Caller must retry on timeout
abstract class LLMService {
  /// Global instance (default: NullLLMService)
  ///
  /// Replace with ClaudeService in bootstrap:
  /// ```dart
  /// LLMService.instance = ClaudeService(apiKey: '...');
  /// await LLMService.instance.initialize();
  /// ```
  static LLMService instance = NullLLMService();

  /// Initialize platform resources and authenticate.
  ///
  /// Call this before using [chat].
  /// May throw if initialization fails.
  Future<void> initialize();

  /// Stream chat completion tokens.
  ///
  /// Returns a stream of text tokens as they're generated by the LLM.
  ///
  /// ## Parameters
  /// - [history]: Previous conversation messages
  /// - [userMessage]: New user message to append
  /// - [systemPrompt]: Optional system prompt (default: service-specific)
  /// - [maxTokens]: Optional max tokens to generate (default: 1024)
  ///
  /// ## Timeout Behavior
  /// - Connection timeout (15s): Throws LLMException if can't connect
  /// - Streaming idle timeout (10s): Throws LLMException if no token received
  ///
  /// ## Example
  /// ```dart
  /// try {
  ///   final buffer = StringBuffer();
  ///   await for (final token in llm.chat(
  ///     history: conversationHistory,
  ///     userMessage: 'Continue the story',
  ///   )) {
  ///     buffer.write(token);
  ///     updateUI(buffer.toString());
  ///   }
  /// } on LLMException catch (e) {
  ///   print('LLM failed: $e');
  ///   // Retry or show error
  /// }
  /// ```
  Stream<String> chat({
    required List<Message> history,
    required String userMessage,
    String? systemPrompt,
    int? maxTokens,
  });

  /// Cleanup resources.
  ///
  /// Always call this when done with the service.
  /// Safe to call multiple times.
  void dispose();

  /// Check if service is ready to use.
  ///
  /// Returns true after successful [initialize], false after [dispose].
  bool get isReady;
}

// ============================================================================
// Message (LLM Chat Message)
// ============================================================================

/// A single message in a conversation.
///
/// Used to build conversation history for [LLMService.chat].
class Message {
  final String role; // 'user', 'assistant', 'system'
  final String content;

  const Message({
    required this.role,
    required this.content,
  });

  Map<String, dynamic> toJson() => {
        'role': role,
        'content': content,
      };

  factory Message.fromJson(Map<String, dynamic> json) => Message(
        role: json['role'] as String,
        content: json['content'] as String,
      );

  /// Create a user message
  factory Message.user(String content) => Message(
        role: 'user',
        content: content,
      );

  /// Create an assistant message
  factory Message.assistant(String content) => Message(
        role: 'assistant',
        content: content,
      );

  /// Create a system message
  factory Message.system(String content) => Message(
        role: 'system',
        content: content,
      );
}

// ============================================================================
// Claude Service (Production Implementation Stub)
// ============================================================================

/// Anthropic Claude API service.
///
/// **STUB IMPLEMENTATION** - Interface complete, implementation needed.
///
/// ## Implementation Checklist
/// - [ ] HTTP SSE streaming to Claude API
/// - [ ] Authentication (API key via x-api-key header)
/// - [ ] Message history management
/// - [ ] System prompts
/// - [ ] Model selection (claude-3-5-sonnet, claude-3-opus, etc.)
/// - [ ] Connection timeout (15s)
/// - [ ] Streaming idle timeout (10s no token)
/// - [ ] Error handling (network, auth, rate limit, content policy)
///
/// ## Configuration
/// ```dart
/// final llm = ClaudeService(
///   apiKey: 'YOUR_API_KEY',
///   model: 'claude-3-5-sonnet-20241022', // Optional
///   defaultMaxTokens: 1024, // Optional
/// );
/// ```
class ClaudeService extends LLMService {
  final String apiKey;
  final String model;
  final int defaultMaxTokens;

  bool _isReady = false;

  ClaudeService({
    required this.apiKey,
    this.model = 'claude-3-5-sonnet-20241022',
    this.defaultMaxTokens = 1024,
  });

  @override
  Future<void> initialize() async {
    // TODO: Implement initialization
    // - Validate API key
    // - Test connection with timeout
    // - Set _isReady = true on success

    print('ClaudeService.initialize() - STUB: Not implemented');
    _isReady = true; // Fake success for now
  }

  @override
  Stream<String> chat({
    required List<Message> history,
    required String userMessage,
    String? systemPrompt,
    int? maxTokens,
  }) async* {
    // TODO: Implement SSE streaming chat
    // 1. POST to /v1/messages with stream=true
    // 2. Parse SSE events (data: {...})
    // 3. Extract tokens from content_block_delta events
    // 4. Apply connection timeout (15s)
    // 5. Apply idle timeout (10s no token)
    // 6. Handle errors (rate limit, content policy, network)

    print('ClaudeService.chat() - STUB: Not implemented');

    // Throw for now
    throw LLMException('ClaudeService not implemented');
  }

  @override
  void dispose() {
    // TODO: Cleanup any resources
    _isReady = false;
    print('ClaudeService.dispose() - STUB: Not implemented');
  }

  @override
  bool get isReady => _isReady;
}

// ============================================================================
// Null LLM Service (Safe Fallback)
// ============================================================================

/// Null Object implementation for LLM service.
///
/// Used when LLM is not configured.
/// Fails gracefully without crashing the app.
class NullLLMService extends LLMService {
  @override
  Future<void> initialize() async {
    print('Warning: LLMService not configured (using NullLLMService)');
  }

  @override
  Stream<String> chat({
    required List<Message> history,
    required String userMessage,
    String? systemPrompt,
    int? maxTokens,
  }) async* {
    print('Warning: LLM unavailable - using NullLLMService');
    throw LLMException('LLM not configured');
  }

  @override
  void dispose() {}

  @override
  bool get isReady => false;
}

// ============================================================================
// Exceptions
// ============================================================================

/// Exception thrown by LLM service.
class LLMException implements Exception {
  final String message;
  final Object? cause;

  LLMException(this.message, {this.cause});

  @override
  String toString() {
    if (cause != null) {
      return 'LLMException: $message (cause: $cause)';
    }
    return 'LLMException: $message';
  }
}
