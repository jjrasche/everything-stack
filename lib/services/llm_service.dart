import 'dart:async';
import 'package:flutter/material.dart';

import 'trainable.dart';

/// Large language model service contract.
///
/// Provides streaming chat completion with conversational context.
///
/// ## Platform Support
/// - Native (Android/iOS/Desktop): ClaudeService (HTTP SSE streaming)
/// - Web: ClaudeService (HTTP SSE streaming via dart:html)
/// - Fallback: NullLLMService (no-op when not configured)
///
/// ## Usage
/// ```dart
/// // Initialize
/// await LLMService.instance.initialize();
///
/// // Chat with streaming response
/// final messages = [
///   Message(role: 'user', content: 'What is the capital of France?'),
/// ];
///
/// await for (final token in LLMService.instance.chat(
///   history: messages,
///   userMessage: 'And its population?',
/// )) {
///   print(token); // Stream tokens as they arrive
/// }
///
/// // Cleanup
/// LLMService.instance.dispose();
/// ```
///
/// ## Timeout Behavior
/// - **Connection timeout**: 15s to establish connection
/// - **Streaming idle timeout**: 10s without token â†’ assume connection stalled
/// - **No automatic retry**: Caller must retry on timeout
abstract class LLMService implements Trainable {
  /// Global instance (default: NullLLMService)
  ///
  /// Replace with ClaudeService in bootstrap:
  /// ```dart
  /// LLMService.instance = ClaudeService(apiKey: '...');
  /// await LLMService.instance.initialize();
  /// ```
  static LLMService instance = NullLLMService();

  /// Initialize platform resources and authenticate.
  ///
  /// Call this before using [chat].
  /// May throw if initialization fails.
  Future<void> initialize();

  /// Stream chat completion tokens.
  ///
  /// Returns a stream of text tokens as they're generated by the LLM.
  ///
  /// ## Parameters
  /// - [history]: Previous conversation messages
  /// - [userMessage]: New user message to append
  /// - [systemPrompt]: Optional system prompt (default: service-specific)
  /// - [maxTokens]: Optional max tokens to generate (default: 1024)
  ///
  /// ## Timeout Behavior
  /// - Connection timeout (15s): Throws LLMException if can't connect
  /// - Streaming idle timeout (10s): Throws LLMException if no token received
  ///
  /// ## Example
  /// ```dart
  /// try {
  ///   final buffer = StringBuffer();
  ///   await for (final token in llm.chat(
  ///     history: conversationHistory,
  ///     userMessage: 'Continue the story',
  ///   )) {
  ///     buffer.write(token);
  ///     updateUI(buffer.toString());
  ///   }
  /// } on LLMException catch (e) {
  ///   print('LLM failed: $e');
  ///   // Retry or show error
  /// }
  /// ```
  Stream<String> chat({
    required List<Message> history,
    required String userMessage,
    String? systemPrompt,
    int? maxTokens,
  });

  /// Non-streaming chat with tool calling support.
  ///
  /// Used for agent workflows where structured tool calls are needed.
  /// Returns complete response (not streamed).
  ///
  /// ## Parameters
  /// - [model]: Model identifier (provider-specific)
  /// - [messages]: Conversation messages (system prompts + user input)
  /// - [tools]: Available tools the LLM can call
  /// - [temperature]: Sampling temperature (0.0-1.0)
  /// - [maxTokens]: Optional max tokens to generate
  ///
  /// ## Usage
  /// ```dart
  /// final response = await llm.chatWithTools(
  ///   model: 'llama-3.3-70b-versatile',
  ///   messages: [
  ///     {'role': 'system', 'content': 'You help manage tasks'},
  ///     {'role': 'user', 'content': 'create a task to buy groceries'},
  ///   ],
  ///   tools: [
  ///     LLMTool(name: 'task.create', description: '...', parameters: {...}),
  ///   ],
  ///   temperature: 0.7,
  /// );
  ///
  /// if (response.toolCalls.isNotEmpty) {
  ///   // Execute tools
  ///   for (final call in response.toolCalls) {
  ///     await executeToolCall(call.toolName, call.params);
  ///   }
  /// }
  /// ```
  Future<LLMResponse> chatWithTools({
    required String model,
    required List<Map<String, dynamic>> messages,
    List<LLMTool>? tools,
    double temperature = 0.7,
    int? maxTokens,
  });

  /// Cleanup resources.
  ///
  /// Always call this when done with the service.
  /// Safe to call multiple times.
  void dispose();

  /// Check if service is ready to use.
  ///
  /// Returns true after successful [initialize], false after [dispose].
  bool get isReady;

  /// Record LLM invocation for training/adaptation
  ///
  /// Called after LLM response completes.
  /// Saves to repository for later feedback and learning.
  @override
  Future<String> recordInvocation(dynamic invocation);

  /// Learn from user feedback (LLM-specific)
  @override
  Future<void> trainFromFeedback(String turnId, {String? userId});

  /// Get current LLM adaptation state
  @override
  Future<Map<String, dynamic>> getAdaptationState({String? userId});

  /// Build UI for LLM feedback
  @override
  Widget buildFeedbackUI(String invocationId);
}

// ============================================================================
// Message (LLM Chat Message)
// ============================================================================

/// A single message in a conversation.
///
/// Used to build conversation history for [LLMService.chat].
class Message {
  final String role; // 'user', 'assistant', 'system'
  final String content;

  const Message({
    required this.role,
    required this.content,
  });

  Map<String, dynamic> toJson() => {
        'role': role,
        'content': content,
      };

  factory Message.fromJson(Map<String, dynamic> json) => Message(
        role: json['role'] as String,
        content: json['content'] as String,
      );

  /// Create a user message
  factory Message.user(String content) => Message(
        role: 'user',
        content: content,
      );

  /// Create an assistant message
  factory Message.assistant(String content) => Message(
        role: 'assistant',
        content: content,
      );

  /// Create a system message
  factory Message.system(String content) => Message(
        role: 'system',
        content: content,
      );
}

// ============================================================================
// LLM Domain Types (Provider-Agnostic)
// ============================================================================

/// Provider-agnostic LLM response with tool calling support.
///
/// Returned by [LLMService.chatWithTools].
/// Contains either text content or tool calls (or both).
class LLMResponse {
  final String id;
  final String? content;
  final List<LLMToolCall> toolCalls;
  final int tokensUsed;

  LLMResponse({
    required this.id,
    this.content,
    required this.toolCalls,
    required this.tokensUsed,
  });

  /// Did the LLM want to call tools?
  bool get hasToolCalls => toolCalls.isNotEmpty;
}

/// Provider-agnostic tool call.
///
/// Represents a single tool invocation requested by the LLM.
/// Parameters are already parsed and validated.
class LLMToolCall {
  final String id;
  final String toolName;
  final Map<String, dynamic> params;

  LLMToolCall({
    required this.id,
    required this.toolName,
    required this.params,
  });
}

/// Provider-agnostic tool definition.
///
/// Passed to LLM to describe available tools.
class LLMTool {
  final String name;
  final String description;
  final Map<String, dynamic> parameters;

  LLMTool({
    required this.name,
    required this.description,
    required this.parameters,
  });

  Map<String, dynamic> toJson() => {
        'type': 'function',
        'function': {
          'name': name,
          'description': description,
          'parameters': parameters,
        },
      };
}

// ============================================================================
// Claude Service (Production Implementation Stub)
// ============================================================================

// ============================================================================
// Null LLM Service (Safe Fallback)
// ============================================================================

/// Null Object implementation for LLM service.
///
/// Used when LLM is not configured.
/// Fails gracefully without crashing the app.
class NullLLMService extends LLMService {
  @override
  Future<void> initialize() async {
    print('Warning: LLMService not configured (using NullLLMService)');
  }

  @override
  Stream<String> chat({
    required List<Message> history,
    required String userMessage,
    String? systemPrompt,
    int? maxTokens,
  }) async* {
    print('Warning: LLM unavailable - using NullLLMService');
    throw LLMException('LLM not configured');
  }

  @override
  Future<LLMResponse> chatWithTools({
    required String model,
    required List<Map<String, dynamic>> messages,
    List<LLMTool>? tools,
    double temperature = 0.7,
    int? maxTokens,
  }) async {
    print('Warning: LLM unavailable - using NullLLMService');
    throw LLMException('LLM not configured');
  }

  @override
  void dispose() {}

  @override
  bool get isReady => false;

  @override
  Future<String> recordInvocation(dynamic invocation) async {
    throw LLMException('LLM not configured');
  }

  @override
  Future<void> trainFromFeedback(String turnId, {String? userId}) async {
    throw LLMException('LLM not configured');
  }

  @override
  Future<Map<String, dynamic>> getAdaptationState({String? userId}) async {
    throw LLMException('LLM not configured');
  }

  @override
  Widget buildFeedbackUI(String invocationId) {
    return Center(child: Text('LLM not configured'));
  }
}

// ============================================================================
// LLM Domain Exceptions (Provider-Agnostic)
// ============================================================================

/// Base exception for LLM service errors.
class LLMException implements Exception {
  final String message;
  final Object? cause;

  LLMException(this.message, {this.cause});

  @override
  String toString() {
    if (cause != null) {
      return 'LLMException: $message (cause: $cause)';
    }
    return 'LLMException: $message';
  }
}

/// LLM request timeout.
///
/// Thrown when connection or streaming idle timeout is exceeded.
class LLMTimeoutException extends LLMException {
  LLMTimeoutException(super.message, {super.cause});
}

/// LLM rate limit exceeded.
///
/// Thrown when provider rate limits are hit.
/// Caller should retry with exponential backoff.
class LLMRateLimitException extends LLMException {
  LLMRateLimitException(super.message, {super.cause});
}

/// LLM server error (5xx).
///
/// Thrown when provider API has server-side errors.
/// Caller may retry after delay.
class LLMServerException extends LLMException {
  LLMServerException(super.message, {super.cause});
}
